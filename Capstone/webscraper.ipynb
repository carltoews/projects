{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraper:\n",
    "A general rule of data science is that no data science project can begin without data. This project follows that principle exactly. Unfortunately, there is no suitable digital corpus of poetry published with text from different eras. So I got to collect my own. What follows is the code for my webscraper, and some initial data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I settled on a website that contained many poems from different eras. The challenge with scraping this website was that both the HTML structure and the URL scheme weren't designed very cleanly, so it wasn't just a simple scrape. In order to collect the URL's for each poem, I first scraped the URL's for each poet's landing page, which led to a page containing the links for each poem. I then scraped the links for each poem, ending with a list of about 32,000 URL's. I then each of those URL's to collect the text of the poem embedded in the HTML code. The Python package used for manipulating HTML code contains a convenient method for getting all the actual text from HTML, but in this situation, that would have pushed the ends of the lines of each poem together, causing me to lose some words. Instead, I decided to manually clean each poem using Python functions to extract the poem text from the HTML code. This process was more time-consuming, but considering the importance of every word in this stylistic analysis, I decided it was certainly worth the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Imports.\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_poets = requests.get('http://www.famouspoetsandpoems.com/poets.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poets = BeautifulSoup(all_poets.text, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting years for each poet\n",
    "In the code below, I scrape the web page that has the years of each poet's life. This was how I was able to separate the poems into eras. It isn't as clean as using the publication date of each poem, but I felt it justified for two reasons:\n",
    "+ Retrieving the publication years of each poem would be a very time-consuming process, assuming that the dates even exist for each poem. \n",
    "+ I made what I believe to be a safe assumption; A poet's style likely follows the style of their era. That said, it is unlikely that they would change their style as the style changes around them. It is more likely that their style stays consistent throughout their life.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poet_years = []\n",
    "\n",
    "for tag in poets.findAll('td'):\n",
    "    try:\n",
    "        if '(' in tag.get_text():\n",
    "            poet_years.append(tag.get_text().strip())\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poet_years2 = [x.strip() for x in poet_years]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poet_years2 = poet_years2[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poet_years = poet_years2[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poet_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poets_and_years = []\n",
    "for i in poet_years:\n",
    "    poets_and_years.append(i.split('('))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poets_and_years_df = pd.DataFrame(poets_and_years, columns=['name', 'number', 'years', 'blech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poets_and_years_df.drop(['blech'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poets_and_years_df.to_csv('poets_years.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping a list of links to each poet's page.\n",
    "The website I'm retrieving my poems from required some clever webscraping. First, I had to collect links to each poet's landing page. These pages each contain links to all of that poet's poems. So after I have all the poet pages, I collect all the links for all of their poems. Then I scrape the actual poem from each of those pages. It amounted to over 32,000 pages that I scraped in total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poet_links = []\n",
    "\n",
    "for tag in poets.findAll('td'):\n",
    "    try:\n",
    "        link = tag.find('a')['href']\n",
    "        if '/poets/' in link:\n",
    "            poet_links.append(link)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "poet_links = list(set(poet_links))\n",
    "poet_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function collects all the links for individual poems from each poet's landing page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_poems(link):\n",
    "    poetry = requests.get(link)\n",
    "    bib_soup = BeautifulSoup(poetry.text, 'lxml')\n",
    "     \n",
    "    poem_links = []\n",
    "    for poems in bib_soup.findAll('td'):\n",
    "        try:\n",
    "            poem = poems.find('a')['href']\n",
    "            if '/poems/' in poem:\n",
    "                poem_links.append(poem)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    poem_links = list(set(poem_links))\n",
    "    poems = []\n",
    "    for poem in poem_links:\n",
    "        poem_link = 'http://www.famouspoetsandpoems.com' + poem      \n",
    "        poems.append(poem_link)\n",
    "    return poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw = 'http://www.famouspoetsandpoems.com'\n",
    "poem_links = []\n",
    "for poet in poet_links:\n",
    "    link = raw + poet + '/poems'\n",
    "    poem_links.append(get_poems(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poem_links[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poem_links_list = [item for sublist in poem_links for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poem_links_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping all the poems from site:\n",
    "Below, I go to each link in the list of 32,000 pages and scrape the actual poem from the page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poems_soup = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for link in poem_links_list[len(poems_soup):]:\n",
    "    url = requests.get(link)\n",
    "    soup = BeautifulSoup(url.text, 'lxml')\n",
    "    poem = soup.find('div', style=\"padding-left:14px;padding-top:20px;font-family:Arial;font-size:13px;\")\n",
    "    for tag in soup('span'):\n",
    "        if 'by' in tag.get_text():\n",
    "            poet = tag.get_text().strip()\n",
    "    poems_soup.append([poem, poet])\n",
    "    if count % 500 == 0:\n",
    "        print count\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creating a DataFrame from all the poems and poets.\n",
    "df_all = pd.DataFrame(poems_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all.to_csv('all_poets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Just a peek at what we're dealing with. It will need cleaning.\n",
    "for i in df_all[0][0:5]:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(poems_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
